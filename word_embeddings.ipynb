{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8fKDWGQlT_hP",
        "A2BZykSRWlSN"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lina-conti/sentiment-analysis/blob/main/word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2BZykSRWlSN"
      },
      "source": [
        "Goal\n",
        "==\n",
        "\n",
        "We are about to design and train a neural system to perform sentiment analysis on film reviews. More precisely, the network will have to output the probability that the input review expresses a positive opinion (overall).\n",
        "\n",
        "The system will be a bag-of-words model using GloVe embeddings. It will have to first average the embeddings of the words of the input review, and then send the result through a simple network that should output a probability.\n",
        "\n",
        "There is a lot of code already written at the beginning of the notebook. It is important that you understand it as you will have to reuse/reproduce it for future work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YowcYrkUOwG"
      },
      "source": [
        "Loading PyTorch is important.\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2dazbs4RArm"
      },
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarks:\n",
        "==\n",
        "*   Follow the instructions very carefully. Do not ignore any comment.\n",
        "*   Comment your code (including the role of all functions and the type of their arguments). A piece of code not appropriately commented can be considered incorrect (irrespectively of whether it works or not).\n",
        "*   Indicate the shape of each tensor that you define.\n",
        "*   Comment all the changes that you make. Any work that is not properly explained can be ignored."
      ],
      "metadata": {
        "id": "keiBW7IR3tIS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEOouXSvWiNA"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is the Large Movie Review Dataset (https://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "Downloading the dataset and pre-processing it might take several minutes, so ask Colab to execute all cells while you are reading the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8AIWygRhI5"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "filename = tmp[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXNSehWtRsXE"
      },
      "source": [
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097EOlPhS07G"
      },
      "source": [
        "# Extracts the dataset.\n",
        "import tarfile\n",
        "tar = tarfile.open(filename)\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp3XCqJ5TS73"
      },
      "source": [
        "import os # Useful library to read files and inspect directories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOruARLhTU4e"
      },
      "source": [
        "# Shows which files and directories are present at the root of the file system.\n",
        "for filename in os.listdir(\".\"):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqYr8dYnRtCx"
      },
      "source": [
        "dataset_root = \"aclImdb\"\n",
        "# Shows which files and directories are present at the root of the dataset directory.\n",
        "for filename in os.listdir(dataset_root):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM9B2NreR-MT"
      },
      "source": [
        "# Shows several reviews.\n",
        "dirname = os.path.join(dataset_root, \"train\", \"pos\") # \"aclImdb/{train|test}/{neg|pos}\"\n",
        "for idx, filename in enumerate(os.listdir(dirname)):\n",
        "  if(idx >= 5): break # Stops after the 5th file.\n",
        "  \n",
        "  print(filename)\n",
        "  with open(os.path.join(dirname, filename)) as f:\n",
        "    review = f.read()\n",
        "    print(review)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcb0jJiaotG"
      },
      "source": [
        "Preprocessing the dataset\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsLxzRGctTt"
      },
      "source": [
        "import nltk # Imports NLTK, an NLP library.\n",
        "nltk.download('punkt') # Loads a module required for tokenization.\n",
        "import collections # This library defines useful data structures. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWWSBBYUoto"
      },
      "source": [
        "newline = \"<br />\" # The reviews sometimes contain this HTLM tag to indicate a line break.\n",
        "def preprocess(text):\n",
        "  text = text.replace(newline, \" \") # Replaces the newline HTML tag with a space.\n",
        "  tokens = nltk.word_tokenize(text); # Converts the text to a list of tokens (strings).\n",
        "  tokens = [token.lower() for token in tokens] # Lowercases all tokens.\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "# Reads and pre-processes the reviews.\n",
        "dataset = {\"train\": [], \"test\": []}\n",
        "binary_classes = {\"neg\": 0, \"pos\": 1}\n",
        "for part_name, l in dataset.items():\n",
        "  for class_name, value in binary_classes.items():\n",
        "    path = os.path.join(dataset_root, part_name, class_name)\n",
        "    print(\"Processing %s...\" % path, end='');\n",
        "    for filename in os.listdir(path):\n",
        "        with open(os.path.join(path, filename)) as f:\n",
        "          review_text = f.read()\n",
        "          review_tokens = preprocess(review_text)\n",
        "          \n",
        "          l.append((review_tokens, value))\n",
        "    print(\" done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlDG9piYiVHL"
      },
      "source": [
        "# Splits the train set into a proper train set and a development/validation set.\n",
        "# 'dataset[\"train\"]' happens to be a list composed of a certain number of negative examples followed by the same number of positive examples.\n",
        "# We are going to use 3/4 of the original train set as our actual train set, and 1/4 as our development set.\n",
        "# We want to keep balanced train and development sets, i.e. for both, half of the reviews should be positive and half should be negative.\n",
        "if(\"dev\" in dataset): print(\"This should only be run once.\")\n",
        "else:\n",
        "  dev_set_half_size = int((len(dataset[\"train\"]) / 4) / 2) # Half of a quarter of the training set size.\n",
        "  dataset[\"dev\"] = dataset[\"train\"][:dev_set_half_size] + dataset[\"train\"][-dev_set_half_size:] # Takes some negative examples at the beginning and some positive ones at the end.\n",
        "  dataset[\"train\"] = dataset[\"train\"][dev_set_half_size:-dev_set_half_size] # Removes the examples used for the development set.\n",
        "\n",
        "  for (part, data) in dataset.items():\n",
        "    class_counts = collections.defaultdict(int)\n",
        "    for (_, p) in data: class_counts[p] += 1\n",
        "    print(f\"{part}: {class_counts}\")\n",
        "  print(\"Train set split into train/dev.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfdibF5dhMIh"
      },
      "source": [
        "Loading the word embeddings\n",
        "==\n",
        "We are going to use GloVe embeddings.\n",
        "\n",
        "All word forms with a frequency below a given threshold are going to be considered unknown forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnkKPGLYxQNR"
      },
      "source": [
        "# Computes the frequency of all word forms in the train set.\n",
        "word_counts = collections.defaultdict(int)\n",
        "for tokens, _ in dataset[\"train\"]:\n",
        "  for token in tokens: word_counts[token] += 1\n",
        "\n",
        "print(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgw19ofeZ23K"
      },
      "source": [
        "# Builds a vocabulary containing only those words present in the train set with a frequency above a given threshold.\n",
        "count_threshold = 4;\n",
        "vocabulary = set()\n",
        "for word, count in word_counts.items():\n",
        "    if(count > count_threshold): vocabulary.add(word)\n",
        "\n",
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi6oZU1kph03"
      },
      "source": [
        "import zipfile\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnhHpcuxckbK"
      },
      "source": [
        "# Returns a dictionary {word[String]: id[Integer]} and a list of Numpy arrays.\n",
        "# `data_path` is the path of the directory containing the GloVe files (if None, 'glove.6B' is used)\n",
        "# `max_size` is the number of word embeddings read (starting from the most frequent; in the GloVe files, the words are sorted)\n",
        "# If `vocabulary` is specified (as a set of strings, or a dictionary from strings to integers), the output vocabulary contains \n",
        "    # the intersection of `vocabulary` and the words with a defined embedding. Otherwise, all words with a defined embedding are used.\n",
        "def get_glove(dim=50, vocabulary=None, max_size=-1, data_path=None):\n",
        "  dimensions = set([50, 100, 200, 300]) # Available dimensions for GloVe 6B\n",
        "  fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip' # (Remember that in GloVe 6B, words are lowercased.)\n",
        "\n",
        "  assert (dim in dimensions), (f'Unavailable GloVe 6B dimension: {dim}.')\n",
        "\n",
        "  if(data_path is None): data_path = 'glove.6B'\n",
        "\n",
        "  # Checks that the data is here, otherwise downloads it.\n",
        "  if(not os.path.isdir(data_path)):\n",
        "    #print('Directory \"%s\" does not exist. Creation.' % data_path)\n",
        "    os.makedirs(data_path)\n",
        "  \n",
        "  glove_weights_file_path = os.path.join(data_path, f'glove.6B.{dim}d.txt')\n",
        "  \n",
        "  if(not os.path.isfile(glove_weights_file_path)):\n",
        "    local_zip_file_path = os.path.join(data_path, os.path.basename(fallback_url))\n",
        "  \n",
        "    if(not os.path.isfile(local_zip_file_path)):\n",
        "      print(f'Retreiving GloVe embeddings from {fallback_url}.')\n",
        "      urllib.request.urlretrieve(fallback_url, local_zip_file_path)\n",
        "    \n",
        "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
        "      print(f'Extracting GloVe embeddings from {local_zip_file_path}.')\n",
        "      z.extractall(path=data_path)\n",
        "  \n",
        "  assert os.path.isfile(glove_weights_file_path), (f\"GloVe file {glove_weights_file_path} not found.\")\n",
        "\n",
        "  # Reads GloVe data.\n",
        "  print('Reading GloVe embeddings.')\n",
        "  new_vocabulary = {} # A dictionary {word[String]: id[Integer]}\n",
        "  embeddings = [] # The list of embeddings (Numpy arrays)\n",
        "  with open(glove_weights_file_path, 'r') as f:\n",
        "    for line in f: # Each line consist of the word followed by a space and all of the coefficients of the vector separated by a space.\n",
        "      values = line.split()\n",
        "\n",
        "      # Here, I'm trying to detect where on the line the word ends and where the vector begins. As in some version(s) of GloVe words can contain spaces, this is not entirely trivial.\n",
        "      vector_part = ' '.join(values[-dim:])\n",
        "      x = line.find(vector_part)\n",
        "      word = line[:(x - 1)]\n",
        "\n",
        "      if((vocabulary is not None) and (not word in vocabulary)): # If a vocabulary was specified and if the word is not it…\n",
        "        continue # …this word is skipped.\n",
        "\n",
        "      new_vocabulary[word] = len(new_vocabulary)\n",
        "      embedding = np.asarray(values[-dim:], dtype=np.float32)\n",
        "      embeddings.append(embedding)\n",
        "\n",
        "      if(len(new_vocabulary) == max_size): break\n",
        "  print('(GloVe embeddings loaded.)')\n",
        "  print()\n",
        "\n",
        "  return (new_vocabulary, embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9nJNVlpZiL"
      },
      "source": [
        "(new_vocabulary, embeddings) = get_glove(dim=50, vocabulary=vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwaov0tWpeFZ"
      },
      "source": [
        "print(len(new_vocabulary)) # Shows the size of the vocabulary.\n",
        "print(new_vocabulary) # Shows each word and its id."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIY-HdiPhgL4"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07xfo-Gux_X"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "class BatchGenerator:\n",
        "  def __init__(self, dataset, vocabulary):\n",
        "    self.dataset = dataset\n",
        "    for part in self.dataset.values(): # Shuffles the dataset so that positive and negative examples are mixed.\n",
        "      np.random.shuffle(part)\n",
        "\n",
        "    self.vocabulary = vocabulary # Dictonary {word[String]: id[Integer]}\n",
        "    self.unknown_word_id = len(vocabulary) # Id for unknown forms\n",
        "    self.padding_idx = len(vocabulary) + 1 # Not all reviews of a given batch will have the same length. \n",
        "                                           # We will \"pad\" shorter reviews with a special token id so that the batch can be represented by a matrix.\n",
        "  \n",
        "  def length(self, data_type='train'):\n",
        "    return len(self.dataset[data_type])\n",
        "\n",
        "  # Returns a random batch.\n",
        "  # Batches are output as a triples (word_ids, polarity, texts). \n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def get_batch(self, batch_size, data_type, subset=None):\n",
        "    data = self.dataset[data_type] # selects the relevant portion of the dataset.\n",
        "    \n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "    instance_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some instance ids.\n",
        "\n",
        "    return self._ids_to_batch(data, instance_ids)\n",
        "\n",
        "  def _ids_to_batch(self, data, instance_ids):\n",
        "    word_ids = [] # Will be a list of lists of word ids (Integer)\n",
        "    polarity = [] # Will be a list of review polarities (Boolean)\n",
        "    texts = [] # Will be a list of lists of words (String)\n",
        "    for instance_id in instance_ids:\n",
        "      text, p = data[instance_id]\n",
        "\n",
        "      word_ids.append([self.vocabulary.get(w, self.unknown_word_id) for w in text])\n",
        "      polarity.append(p)\n",
        "      texts.append(text)\n",
        "    \n",
        "    # Padding\n",
        "    self.pad(word_ids)\n",
        "\n",
        "    word_ids = torch.tensor(word_ids, dtype=torch.long) # Conversion to a tensor\n",
        "    polarity = torch.tensor(polarity, dtype=torch.bool) # Conversion to a tensor\n",
        "\n",
        "    return (word_ids, polarity, texts) # We don't really need `texts` but it might be useful to debug the system.\n",
        "  \n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, word_ids):\n",
        "    max_length = max([len(s) for s in word_ids])\n",
        "    for s in word_ids: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "  \n",
        "  # Returns a generator of batches for a full epoch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def all_batches(self, batch_size, data_type=\"train\", subset=None):\n",
        "    data = self.dataset[data_type]\n",
        "    \n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size').\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "      i += batch_size\n",
        "    \n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "  \n",
        "  # Turns a list of arbitrary pre-processed texts into a batch.\n",
        "  # This function will be used to infer the polarity of a unannotated review.\n",
        "  def turn_into_batch(self, texts):\n",
        "    word_ids = [[self.vocabulary.get(w, self.unknown_word_id) for w in text] for text in texts]\n",
        "    self.pad(word_ids)\n",
        "    return torch.tensor(word_ids, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(dataset=dataset, vocabulary=new_vocabulary)\n",
        "print(batch_generator.length('train')) # Prints the number of instance in the train set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35rEb8l0_Kd"
      },
      "source": [
        "tmp = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(tmp[0]) # Prints the matrix of token ids.\n",
        "print(tmp[1]) # Prints the vector of polarities.\n",
        "print(tmp[2]) # Prints the list of reviews."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp2VOTBzFLb9"
      },
      "source": [
        "len(list(batch_generator.all_batches(batch_size=3, data_type=\"train\"))) # Number of batches in the training set for batches of size 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTuIZoIhkTW"
      },
      "source": [
        "The model\n",
        "==\n",
        "Please, **pay attention to all comments**.\n",
        "They contain useful information.\n",
        "\n",
        "You might wonder what the \".to\" method of tensors is for.\n",
        "To execute the neural network faster, we will run it on a GPU instead of a CPU.\n",
        "To do so, data and parameters should be sent on the GPU, which is done by using the \".to\" method.\n",
        "This is possible if the parameters of the notebook allow it (i.e. if Edit/Notebook Settings/Hardware Accelerator is \"GPU\"), which should be the case.\n",
        "If you implement things correctly, you should not need to add any call to the \".to\" method here (and only one or two later during the training process).\n",
        "\n",
        "For your system to be efficient, you should **never loop over a tensor** whenever it is possible to do otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE6mbJ9Q2nUy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class SentimentClassifier(torch.nn.Module):\n",
        "  # embeddings: list of Numpy arrays\n",
        "  # hidden_sizes: list of the size (Integer) of each hidden layer; there may be 0 or more hidden layers\n",
        "  # freeze_embeddings: boolean; indicates whether the embeddings should be frozen (i.e., not fine-tuned) during training\n",
        "  # device: string; indicates on which type of hardware PyTorch computation should be run\n",
        "  def __init__(self, embeddings, hidden_sizes, freeze_embeddings=True, device='cpu'):\n",
        "    embeddings = list(embeddings) # Creates a copy of the list of embeddings, so we can add or remove entries without affecting the original list.\n",
        "    super().__init__() # Calls the constructor of the parent class. Usually, this is necessary when creating a custom module.\n",
        "\n",
        "    #################\n",
        "    # (i) define a vector for unknown forms (the average of actual word embeddings) and a vector for the padding token (full of 0·s) \n",
        "    unk_embedding = np.average(embeddings, axis=0)  # numpy array of shape (50)\n",
        "    embeddings_size = len(unk_embedding)    # 50\n",
        "    pad_embedding = np.zeros(embeddings_size)    # numpy array of size 50\n",
        "\n",
        "    # (ii) define an embedding layer 'self.embeddings' using torch.nn.Embedding.from_pretrained and without forgeting to use the 'freeze' and 'padding_idx' arguments \n",
        "    # (this last argument is used to keep the padding embedding at 0 even when fine-tuning the other embeddings).\n",
        "    # The following error (if you get it) indicates that the value provided for 'padding_idx' does not correspond to any embedding in the matrix that you provide \n",
        "    # (in other words, the matrix is likely to be incomplete): \"Padding_idx must be within num_embeddings\".\n",
        "    embeddings_tensor = torch.Tensor(np.array(embeddings + [unk_embedding, pad_embedding]))   # Shape (vocabulary_size, embeddings_size)\n",
        "    # Converting to a numpy array and then to a tensor is faster.\n",
        "    self.padding_idx = len(embeddings) + 1  # We will need this later so we can ignore the padding tokens.\n",
        "    self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, padding_idx=len(embeddings)+1, freeze=freeze_embeddings)\n",
        " \n",
        "    #################\n",
        "    self.embeddings = self.embeddings.to(device) # Sends the word embeddings to 'device', which is potentially a GPU.\n",
        "\n",
        "    #################\n",
        "    # Here you have to define self.main_part, the network that computes a probability for any review given as input (represented as the average of the embeddings of the tokens).\n",
        "    # The number of hidden layers is determined by 'hidden_sizes, which is a list of integers describing the (output) size of each of them.\n",
        "    # Use torch.nn.Linear to build linear layers.\n",
        "    # torch.nn.Sequential takes one argument per module and not a list of modules as argument, \n",
        "    # but if 'modules' is a list of modules, 'torch.nn.Sequential(*modules)' (with the star notation) works.\n",
        "\n",
        "    modules = []  # a list of modules\n",
        "    previous_size = embeddings_size # size of the input\n",
        "    for size in hidden_sizes:\n",
        "        # Each hidden layer is composed of a linear layer and an activation function (here, ReLu)\n",
        "        # previous_size is the size of the input of the hidden layer and size the size of its output\n",
        "        modules.append(torch.nn.Linear(previous_size, size))\n",
        "        modules.append(torch.nn.ReLU())\n",
        "        previous_size = size\n",
        "    # The last layer must output a single scalar\n",
        "    modules.append(torch.nn.Linear(previous_size, 1))\n",
        "    # This single scalar is run through the sigmoid function to get the probability of the positive class (between 0 and 1)\n",
        "    modules.append(torch.nn.Sigmoid())\n",
        "\n",
        "    self.main_part = torch.nn.Sequential(*modules)  # Puts the modules together into a neural network.\n",
        "\n",
        "    #################\n",
        "    self.main_part = self.main_part.to(device) # Sends the network to 'device', which is potentially a GPU.\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  # 'batch' is 2D tensor (i.e. a matrix) of word ids (Integer).\n",
        "  def forward(self, batch):\n",
        "    #################\n",
        "    # (i) turn 'batch' into a matrix of embeddings (i.e. a tensor of rank 3)\n",
        "    # batch is a tensor of shape (batch_size, number_of_tokens)\n",
        "    embeddings_matrix = self.embeddings(batch) # Gets the embeddings corresponding to the indices in batch\n",
        "    # Shape: (batch_size, number_of_tokens, embedding_size)\n",
        "\n",
        "    # (ii) average all embeddings for a given review while being careful not to take into account padding vectors\n",
        "    tokens_sum = torch.sum(embeddings_matrix, axis=1) # Sums the embeddings of all tokens (0 for the padding token)\n",
        "    # Shape: (batch_size, embedding_size)\n",
        "    mask = batch == self.padding_idx \n",
        "    # Shape: (batch_size, number_of_tokens)\n",
        "    examples_length = batch.shape[1] - torch.sum(mask, axis=1) # For each review: length of the longest review in the batch - number of padding tokens in review\n",
        "    # Shape: (batch_size)\n",
        "    examples_length = torch.unsqueeze(examples_length, 1) # tokens_sum and examples_length have to be of the same dimension so we can divide one for the other\n",
        "    # Shape: (batch_size, 1)\n",
        "    input = tokens_sum / examples_length    # tensor of shape (batch_size, embedding_size)\n",
        "\n",
        "    # (iii) send these bag-of-words representations to the network.\n",
        "    # Return a tensor of shape (batch size) instead of (batch size, 1).\n",
        "    return self.main_part(input).squeeze()     \n",
        "\n",
        "    # Once you think the function works, check that the presence of padding ids does NOT impact the result in any way\n",
        "    # (i.e. the same probability should be computed for a given review independently of the number of padding ids). \n",
        "    #################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpqcxScW4Afb"
      },
      "source": [
        "model = SentimentClassifier(embeddings, hidden_sizes=[100], freeze_embeddings=True)\n",
        "batch = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(model(batch[0])) # This output (its shape) should be checked."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmL7aPacmNVh"
      },
      "source": [
        "# Function that computes the accuracy of the model on a given part of the dataset.\n",
        "evaluation_batch_size = 256\n",
        "def evaluation(model, data_type, subset=None):\n",
        "  nb_correct = 0\n",
        "  total = 0\n",
        "  for batch in batch_generator.all_batches(evaluation_batch_size, data_type=data_type, subset=subset):\n",
        "    prob = model(batch[0].to(model.device)) # Forward pass\n",
        "    answer = (prob > 0.5) # Shape: (evaluation_batch_size, 1)\n",
        "    nb_correct += (answer == batch[1].to(model.device)).sum().item()\n",
        "    total += batch[0].shape[0]\n",
        "      \n",
        "  accuracy = (nb_correct / total)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_gHaFzrOaj"
      },
      "source": [
        "Training\n",
        "==\n",
        "Once everything works, try to find better hyperparameters.\n",
        "The goal is to maximise the accuracy on the development set.\n",
        "Feel also free to improve the model or the training process.\n",
        "Graphs used for visualising the training process are also much welcome.\n",
        "(These instructions apply to all future TPs as well.)\n",
        "\n",
        "You should document in a text cell as much as possible what you do and, when relevant, how it affects the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOMCyVPK6BD8"
      },
      "source": [
        "model = SentimentClassifier(embeddings, hidden_sizes=[20,10], freeze_embeddings=False, device='cuda')\n",
        "\n",
        "# Tests the model on a couple of instance before training.\n",
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "print(model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\"]]).to(model.device)))\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.004\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=l2_reg) \n",
        "# Once the backward propagation has been done, just call the 'step' method (with no argument) of this object to update the parameters.\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 20\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "loss_list = [] # The average loss for each epoch will be used to plot the learning curve\n",
        "train_accuracies = [] # The accuracy on the train set for each epoch will be used to plot the learning curve\n",
        "dev_accuracies = [] # The accuracy on the dev set for each epoch will be used to plot the learning curve\n",
        "loss_fn = torch.nn.BCELoss(reduction='mean') # This loss function is adapted to the binary case\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells PyTorch that we are in training mode (can be useful if dropout is used, for instance).\n",
        "  \n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "  \n",
        "  batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset) # The batch to train on at this iteration.\n",
        "  \n",
        "  ###################\n",
        "  # (i) compute the prediction of the model (you might want to use \".to(model.device)\" on the input of the model)\n",
        "  prob = model(batch[0].to(model.device)) # Shape: (batch_size)\n",
        "\n",
        "  # (ii) compute the loss (use an average over the batch) \n",
        "  targets = batch[1].float().to(model.device) # Casts booleans to floats so we can compare them to probabilities.\n",
        "  loss = loss_fn(prob, targets)\n",
        "\n",
        "  # (iii) call \"backward\" on the loss \n",
        "  loss.backward()\n",
        "\n",
        "  # (iv) store the loss in \"epoch_loss\".\n",
        "  epoch_loss.append(loss.item())\n",
        "\n",
        "  ###################\n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size): # If this iteration corresponds to the end of an epoch.\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "    loss_list.append(sum(epoch_loss) / len(epoch_loss))\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      accuracy = evaluation(model, \"train\")\n",
        "      print(f\"Accuracy on the train set: {accuracy}.\")\n",
        "      train_accuracies.append(accuracy)\n",
        "\n",
        "      accuracy = evaluation(model, \"dev\")\n",
        "      print(f\"Accuracy on the dev set: {accuracy}.\")\n",
        "      dev_accuracies.append(accuracy)\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graphs for visualizing the training process\n",
        "\n",
        "Loss decreases and accuracy increases over time, so we know the system is learning."
      ],
      "metadata": {
        "id": "HoN0zevV0GJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Makes it easier to obtain the graphs in fewer lines of code.\n",
        "\n",
        "# Plotting how the average loss evolves with each epoch\n",
        "df = pd.DataFrame({\"loss\": loss_list})\n",
        "df.plot(xlabel='Epoch', ylabel='Accuracy', title='Loss learning curve', xticks=range(len(loss_list)))\n",
        "\n",
        "# Plotting how the accuracy on the train and dev sets evolves with each epoch\n",
        "df = pd.DataFrame({\"train set\": train_accuracies, \"devlopment set\": dev_accuracies})\n",
        "df.plot(xlabel='Epoch', ylabel='Accuracy', title='Accuracy learning curve', xticks=range(len(loss_list)))"
      ],
      "metadata": {
        "id": "g5yxWYiVrKvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh4na1hivgXP"
      },
      "source": [
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\", \"Bad.\", \"Not bad!\"]]).to(model.device)) > 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the use of a scheduler\n",
        "\n",
        "I tried to use a scheduler to see whether it would improve performance or make my model learn faster. I first tested [ExponentialLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR) and then [OneCycleLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR), which is similar to what is recommended [here](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8), although I didn't do a learning rate range test to determine the best value for the maximum learning rate. However, at least with this hyperparameter combinations, I obtained a lower or similar performance with the scheduler, so I decided to leave it aside and focus on other hyperparameters for now."
      ],
      "metadata": {
        "id": "KSXuFR5MnmIW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtvUkArgwM_I"
      },
      "source": [
        "# With exponential scheduler\n",
        "\n",
        "model = SentimentClassifier(embeddings, hidden_sizes=[20,10], freeze_embeddings=False, device='cuda')\n",
        "\n",
        "# Tests the model on a couple of instance before training.\n",
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "print(model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\"]]).to(model.device)))\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.004\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=l2_reg)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  # Instanciates the scheduler, the gamma value is the one used in th Pytorch example\n",
        "# Once the backward propagation has been done, just call the 'step' method (with no argument) of this object to update the parameters.\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 20\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "loss_list = [] # The average loss for each epoch will be used to plot the learning curve\n",
        "train_accuracies = [] # The accuracy on the train set for each epoch will be used to plot the learning curve\n",
        "dev_accuracies = [] # The accuracy on the dev set for each epoch will be used to plot the learning curve\n",
        "loss_fn = torch.nn.BCELoss(reduction='mean') # This loss function is adapted to the binary case\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells PyTorch that we are in training mode (can be useful if dropout is used, for instance).\n",
        "  \n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "  \n",
        "  batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset) # The batch to train on at this iteration.\n",
        "  \n",
        "  ###################\n",
        "  # (i) compute the prediction of the model (you might want to use \".to(model.device)\" on the input of the model)\n",
        "  prob = model(batch[0].to(model.device)) # Shape: (batch_size)\n",
        "\n",
        "  # (ii) compute the loss (use an average over the batch) \n",
        "  targets = batch[1].float().to(model.device) # Casts booleans to floats so we can compare them to probabilities.\n",
        "  loss = loss_fn(prob, targets)\n",
        "\n",
        "  # (iii) call \"backward\" on the loss \n",
        "  loss.backward()\n",
        "\n",
        "  # (iv) store the loss in \"epoch_loss\".\n",
        "  epoch_loss.append(loss.item())\n",
        "\n",
        "  ###################\n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size): # If this iteration corresponds to the end of an epoch.\n",
        "    scheduler.step()  # At the end of each epoch\n",
        "\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "    loss_list.append(sum(epoch_loss) / len(epoch_loss))\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      accuracy = evaluation(model, \"train\")\n",
        "      print(f\"Accuracy on the train set: {accuracy}.\")\n",
        "      train_accuracies.append(accuracy)\n",
        "\n",
        "      accuracy = evaluation(model, \"dev\")\n",
        "      print(f\"Accuracy on the dev set: {accuracy}.\")\n",
        "      dev_accuracies.append(accuracy)\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQzJBATXnzcT"
      },
      "source": [
        "# With one cylce learning rate scheduler\n",
        "\n",
        "model = SentimentClassifier(embeddings, hidden_sizes=[20,10], freeze_embeddings=False, device='cuda')\n",
        "\n",
        "# Tests the model on a couple of instance before training.\n",
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "print(model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\"]]).to(model.device)))\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.004\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=l2_reg)\n",
        "# Once the backward propagation has been done, just call the 'step' method (with no argument) of this object to update the parameters.\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 20\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "loss_list = [] # The average loss for each epoch will be used to plot the learning curve\n",
        "train_accuracies = [] # The accuracy on the train set for each epoch will be used to plot the learning curve\n",
        "dev_accuracies = [] # The accuracy on the dev set for each epoch will be used to plot the learning curve\n",
        "loss_fn = torch.nn.BCELoss(reduction='mean') # This loss function is adapted to the binary case\n",
        "\n",
        "# The scheduler is created here because epoch_size, batch_size and nb_epoch must be defined\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=int(epoch_size/batch_size)+1, epochs=nb_epoch)\n",
        "\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells PyTorch that we are in training mode (can be useful if dropout is used, for instance).\n",
        "  \n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "  \n",
        "  batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset) # The batch to train on at this iteration.\n",
        "  \n",
        "  ###################\n",
        "  # (i) compute the prediction of the model (you might want to use \".to(model.device)\" on the input of the model)\n",
        "  prob = model(batch[0].to(model.device)) # Shape: (batch_size)\n",
        "\n",
        "  # (ii) compute the loss (use an average over the batch) \n",
        "  targets = batch[1].float().to(model.device) # Casts booleans to floats so we can compare them to probabilities.\n",
        "  loss = loss_fn(prob, targets)\n",
        "\n",
        "  # (iii) call \"backward\" on the loss \n",
        "  loss.backward()\n",
        "\n",
        "  # (iv) store the loss in \"epoch_loss\".\n",
        "  epoch_loss.append(loss.item())\n",
        "\n",
        "  ###################\n",
        "  optimizer.step() # Updates the parameters.\n",
        "  scheduler.step() # After each batch is processed\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size): # If this iteration corresponds to the end of an epoch.\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "    loss_list.append(sum(epoch_loss) / len(epoch_loss))\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      accuracy = evaluation(model, \"train\")\n",
        "      print(f\"Accuracy on the train set: {accuracy}.\")\n",
        "      train_accuracies.append(accuracy)\n",
        "\n",
        "      accuracy = evaluation(model, \"dev\")\n",
        "      print(f\"Accuracy on the dev set: {accuracy}.\")\n",
        "      dev_accuracies.append(accuracy)\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "Looking for the set of hyperparameters that maximises the accuracy on the development set.\n",
        "\n",
        "## Early stopping\n",
        "\n",
        "- Number of epochs\n",
        "\n",
        "From the learning curve computed previously, I was not completely sure whether we had reached a plateau or whether performance on the development set could still be increased (slowly) by training for more epochs. So, I implemented early stopping. With a patience of 5, I trained until epoch 31 and was able to get 1.5 percent points more on the accuracy, without changing the other hyperparameters.\n",
        "\n",
        "## Grid search\n",
        "\n",
        "For some other hyperparameters, I used grid search to look for the best combination of values. Among the values I have tested, the best combination was 'learning rate': 0.004, 'weight decay': 0.0001, 'momentum': 0.99, 'embedding size': 300, with and accuracy of 88,432%.\n",
        "\n",
        "- Learning rate\n",
        "- Weight decay\n",
        "- Momentum\n",
        "- Embeddings size\n",
        "\n",
        "I could also have changed the batch size, but I thought that this would have bigger impact on the efficiency of training than on the accuracy, so, I didn't change it for now. Another possibility would have been to test other architectures (a different number of layers or different layer sizes)."
      ],
      "metadata": {
        "id": "-40Pd3WfsXV6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAFYqqaH7Tx7"
      },
      "source": [
        "(vocabulary_50, embeddings_50) = get_glove(dim=50, vocabulary=vocabulary)\n",
        "batch_generator_500 = BatchGenerator(dataset=dataset, vocabulary=vocabulary_50)\n",
        "\n",
        "(vocabulary_100, embeddings_100) = get_glove(dim=100, vocabulary=vocabulary)\n",
        "batch_generator_100 = BatchGenerator(dataset=dataset, vocabulary=vocabulary_100)\n",
        "\n",
        "(vocabulary_200, embeddings_200) = get_glove(dim=200, vocabulary=vocabulary)\n",
        "batch_generator_200 = BatchGenerator(dataset=dataset, vocabulary=vocabulary_200)\n",
        "\n",
        "(vocabulary_300, embeddings_300) = get_glove(dim=300, vocabulary=vocabulary)\n",
        "batch_generator_300 = BatchGenerator(dataset=dataset, vocabulary=vocabulary_300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.4, 0.04, 0.004, 0.0004]\n",
        "l2_regs = [0.0001, 0.00001, 0.000001, 0]\n",
        "momentums = [0, 0.6, 0.8, 0.99]\n",
        "embedding_sizes = [(batch_generator_100, embeddings_100, 100), (batch_generator_200, embeddings_200, 200), (batch_generator_300, embeddings_300, 300), (batch_generator_50, embeddings_50, 50)]"
      ],
      "metadata": {
        "id": "1pV5VC6V66wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zc-QqVbGa3u"
      },
      "source": [
        "# Training procedure\n",
        "\n",
        "# Hyperparameter tuning\n",
        "\n",
        "best_hyperparameters = {} # for comparing hyperparameter values\n",
        "best_accuracy = 0 # for comparing hyperparameter values\n",
        "for learning_rate in learning_rates:\n",
        "  for l2_reg in l2_regs:\n",
        "    for momentum in momentums:\n",
        "      for batch_generator, embeddings, embed_dim in embedding_sizes:\n",
        "        model = SentimentClassifier(embeddings, hidden_sizes=[20,10], freeze_embeddings=False, device='cuda') # Must be restarted every time\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) \n",
        "        patience = 5 # nb of epochs with decreased performance to wait for before interrupting training\n",
        "\n",
        "        # Once the backward propagation has been done, just call the 'step' method (with no argument) of this object to update the parameters.\n",
        "        batch_size = 64\n",
        "        subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "        epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n",
        "\n",
        "        dev_accuracies = [] # Used for early stopping\n",
        "        nb_epoch = 50\n",
        "        epoch_id = 0 # Id of the current epoch\n",
        "        instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "        epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "        loss_fn = torch.nn.BCELoss(reduction='mean') # This loss function is adapted to the binary case\n",
        "        while(epoch_id < nb_epoch):\n",
        "          model.train() # Tells PyTorch that we are in training mode (can be useful if dropout is used, for instance).\n",
        "          \n",
        "          model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "          \n",
        "          batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset) # The batch to train on at this iteration.\n",
        "          \n",
        "          ###################\n",
        "          # (i) compute the prediction of the model (you might want to use \".to(model.device)\" on the input of the model)\n",
        "          prob = model(batch[0].to(model.device)) # Shape: (batch_size)\n",
        "\n",
        "          # (ii) compute the loss (use an average over the batch) \n",
        "          targets = batch[1].float().to(model.device) # Casts booleans to floats so we can compare them to probabilities.\n",
        "          loss = loss_fn(prob, targets)\n",
        "\n",
        "          # (iii) call \"backward\" on the loss \n",
        "          loss.backward()\n",
        "\n",
        "          # (iv) store the loss in \"epoch_loss\".\n",
        "          epoch_loss.append(loss.item())\n",
        "\n",
        "          ###################\n",
        "          optimizer.step() # Updates the parameters.\n",
        "\n",
        "          instances_processed += batch_size\n",
        "          if(instances_processed > epoch_size): # If this iteration corresponds to the end of an epoch.\n",
        "\n",
        "            # Evaluation\n",
        "            model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "            with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "              accuracy = evaluation(model, \"dev\")\n",
        "              dev_accuracies.append(accuracy)\n",
        "              if accuracy > best_accuracy:\n",
        "                print(f\"Best accuracy so far: {accuracy}!\")\n",
        "                best_accuracy = accuracy\n",
        "                best_hyperparameters[\"learning rate\"] = learning_rate\n",
        "                best_hyperparameters[\"weight decay\"] = l2_reg\n",
        "                best_hyperparameters[\"momentum\"] = momentum\n",
        "                best_hyperparameters[\"embedding size\"] = embed_dim\n",
        "\n",
        "              # Early stopping procedure\n",
        "              if epoch_id >= patience:\n",
        "                early_stop = True\n",
        "                # We are testing whether the accuracy on the dev set obtained patience-1 epochs ago is better then all the following ones\n",
        "                accuracy_to_test = dev_accuracies[-patience-1]  # Accuracy obtained patience+1 epochs ago\n",
        "                for accuracy in dev_accuracies[-patience:]: # All following accuracies\n",
        "                  if accuracy_to_test < accuracy:\n",
        "                    early_stop = False\n",
        "                if early_stop:\n",
        "                  break\n",
        "\n",
        "            epoch_id += 1\n",
        "            instances_processed -= epoch_size\n",
        "            epoch_loss = []\n",
        "        \n",
        "        print(f\"Learning rate {learning_rate}, weight decay {l2_reg}, momentum {momentum}, embedding size: {embed_dim}\")\n",
        "        print(f\"Stopped at epoch {epoch_id}\")\n",
        "        print(f\"Best accuracy: {max(dev_accuracies)}\\n\")\n",
        "\n",
        "print(best_hyperparameters)\n",
        "print(best_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vMKImIJu4Iqm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}